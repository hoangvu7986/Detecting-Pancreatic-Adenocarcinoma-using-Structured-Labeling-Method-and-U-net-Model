{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangvu7986/Detecting-Pancreatic-Adenocarcinoma-using-Structured-Labeling-Method-and-U-net-Model/blob/main/Copy_of_thesis_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pRPUUk9NloAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nibabel"
      ],
      "metadata": {
        "id": "bZoHuZaD97O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import library"
      ],
      "metadata": {
        "id": "mJc9Au0mk7Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob as gl\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "wecvKW9wvaY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation"
      ],
      "metadata": {
        "id": "Ru5_hpZ7lDTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(image, mask, flip_ud=False, flip_lr=False):\n",
        "  if(np.random.random() > 0.5):\n",
        "    flip_ud = True\n",
        "    # Flip vertically\n",
        "    if flip_ud and np.random.random() > 0.5:\n",
        "        image = cv2.flip(image, 0)\n",
        "        mask = cv2.flip(mask, 0)\n",
        "    return image, mask\n",
        "  elif (np.random.random() < 0.5):\n",
        "    flip_lr = True\n",
        "    # Flip horizontally\n",
        "    if flip_lr and np.random.random() > 0.5:\n",
        "        image = cv2.flip(image, 1)\n",
        "        mask = cv2.flip(mask, 1)\n",
        "    return image, mask\n",
        "  else:\n",
        "    return image, mask\n"
      ],
      "metadata": {
        "id": "go0uzkhHW59p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 5):\n",
        "  image = np.load('/content/drive/MyDrive/research/image/image_'+str(i)+'.npy')\n",
        "  label = np.load('/content/drive/MyDrive/research/label/label_'+str(i)+'.npy')\n",
        "  img, lbl = augment_data(image, label)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.subplot(2, 2, 1)\n",
        "  plt.imshow(image[:, :])\n",
        "  plt.subplot(2, 2, 2)\n",
        "  plt.imshow(label[:, :])\n",
        "  plt.subplot(2, 2, 3)\n",
        "  plt.imshow(img[:, :])\n",
        "  plt.subplot(2, 2, 4)\n",
        "  plt.imshow(lbl[:, :])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ohMpGsf5YalC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dice loss function"
      ],
      "metadata": {
        "id": "OBoPPuwflKgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    \"\"\"\n",
        "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
        "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
        "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
        "    \"\"\"\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)"
      ],
      "metadata": {
        "id": "LDAuQPdYwfiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuning U-net model"
      ],
      "metadata": {
        "id": "h2t82MSxlPFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unet(height, width, channel, n_class):\n",
        "    inputs = tf.keras.layers.Input((height, width, channel))\n",
        "\n",
        "    # Encoder part\n",
        "    c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "    c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "    c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "    c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "    c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = tf.keras.layers.MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "    c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    # Decode part\n",
        "    u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "    c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = tf.keras.layers.Dropout(0.1)(c6)\n",
        "    c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "    c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
        "    c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "    c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "    c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = tf.keras.layers.concatenate([u9, c1])\n",
        "    c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "    c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(n_class, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "Pdfi4q0B6Hgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet(128, 128, 1, 3)\n",
        "print(model.input_shape)\n",
        "print(model.output_shape)"
      ],
      "metadata": {
        "id": "NpMs-2EhqNDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot-encoding for mask and create an pipline"
      ],
      "metadata": {
        "id": "EvWlbe9tlaHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(mask):\n",
        "    one_hot_mask = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "    for i in range(mask.shape[0]):\n",
        "        for j in range(mask.shape[1]):\n",
        "            if mask[i][j] == 0:\n",
        "                one_hot_mask[i][j][0] = 1\n",
        "            elif mask[i][j] == 1:\n",
        "                one_hot_mask[i][j][1] = 1\n",
        "            elif mask[i][j] == 2:\n",
        "                one_hot_mask[i][j][2] = 1\n",
        "    return one_hot_mask\n",
        "\n",
        "def generator(dir_images, dir_masks, batch_size, train=False):\n",
        "    while True:\n",
        "        for i in range(0, len(dir_images), batch_size):\n",
        "            image_array = []\n",
        "            mask_array = []\n",
        "            for i, j in zip(dir_images[i:i + batch_size], dir_masks[i:i + batch_size]):\n",
        "              load_image = np.load(i)\n",
        "              load_mask = np.load(j)\n",
        "              if(train == True):\n",
        "                img, msk = augment_data(load_image, load_mask, flip_ud=False, flip_lr=False)\n",
        "                image_array.append(img)\n",
        "                mask_array.append(msk)\n",
        "              else:\n",
        "                image_array.append(load_image)\n",
        "                mask_array.append(load_mask)\n",
        "\n",
        "            x_batch = [np.expand_dims(cv2.resize(image, (128, 128)), axis = -1) for image in image_array]\n",
        "            y_batch = [cv2.resize(mask, (128, 128)) for mask in mask_array]\n",
        "            one_hot_y_batch = [one_hot_encode(mask) for mask in y_batch]\n",
        "            yield (np.array(x_batch), np.array(one_hot_y_batch))"
      ],
      "metadata": {
        "id": "9IWvC5cNy2lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_image = '/content/drive/MyDrive/research/image/*'\n",
        "path_label = '/content/drive/MyDrive/research/label/*'"
      ],
      "metadata": {
        "id": "LWLUyAPB7U2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_images = []\n",
        "dir_masks = []\n",
        "for i in sorted(gl.glob(path_image)):\n",
        "  dir_images.append(i)\n",
        "for i in sorted(gl.glob(path_label)):\n",
        "  dir_masks.append(i)"
      ],
      "metadata": {
        "id": "SWDnMc3v8HWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dir_images))\n",
        "print(len(dir_masks))"
      ],
      "metadata": {
        "id": "-fvy3a-Ks3FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle image and mask corresponding"
      ],
      "metadata": {
        "id": "7kfdSf1ilzRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = list(range(len(dir_images)))\n",
        "random.shuffle(indices)"
      ],
      "metadata": {
        "id": "LirbW0dm-70m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_dir_images = [dir_images[i] for i in indices]\n",
        "shuffle_dir_masks = [dir_masks[i] for i in indices]"
      ],
      "metadata": {
        "id": "QFJ0yG5aAwqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in zip(shuffle_dir_images, shuffle_dir_masks):\n",
        "  print(i, j)"
      ],
      "metadata": {
        "id": "oEO8yPe6BMv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#divide 20% for test set and 80% for valid, train set\n",
        "train_images = shuffle_dir_images[:int(0.6 * len(shuffle_dir_images))]\n",
        "val_images = shuffle_dir_images[int(0.6 * len(shuffle_dir_images)):int(0.8 * len(shuffle_dir_images))]\n",
        "test_images = shuffle_dir_images[int(0.8 * len(shuffle_dir_images)):]\n",
        "\n",
        "train_masks = shuffle_dir_masks[:int(0.6 * len(shuffle_dir_masks))]\n",
        "val_masks = shuffle_dir_masks[int(0.6 * len(shuffle_dir_masks)):int(0.8 * len(shuffle_dir_masks))]\n",
        "test_masks = shuffle_dir_masks[int(0.8 * len(shuffle_dir_masks)):]\n",
        "\n",
        "train_gen = generator(train_images, train_masks, batch_size=32, train=True)\n",
        "val_gen = generator(val_images, val_masks, batch_size=32, train=False)\n",
        "test_gen = generator(test_images, test_masks, batch_size=32, train=False)"
      ],
      "metadata": {
        "id": "rCqy7FmoAdwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = next(train_gen)\n",
        "print(x_batch.shape)\n",
        "print(y_batch.shape)"
      ],
      "metadata": {
        "id": "gyJFlfq8tQwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checkpoint and Early stopping\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/research/next_final.h5', save_best_only=True)\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "#early_stop= tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', mode='min')"
      ],
      "metadata": {
        "id": "LPenyWv_Bgla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = dice_coef_loss, metrics = [dice_coef])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "EqN0atNEBg2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "history = model.fit(train_gen,\n",
        "                    steps_per_epoch=len(train_images) // 32,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_gen,\n",
        "                    callbacks = [checkpointer],\n",
        "                    validation_steps=len(val_images) // 32,\n",
        "                    shuffle = True)"
      ],
      "metadata": {
        "id": "7JXPrV35CG8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize result\n",
        "acc = history.history['dice_coef']\n",
        "val_acc = history.history['val_dice_coef']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(50)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.plot(epochs_range, acc, label='Training dice_coef')\n",
        "plt.plot(epochs_range, val_acc, label='Validation dice_coef')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation dice_coef')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2hqwC-UfCewD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import csv library and save test set for check accuracy"
      ],
      "metadata": {
        "id": "Hd63ncHfmbvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "csv_file = '/content/drive/MyDrive/research/save_test.csv'"
      ],
      "metadata": {
        "id": "GB15CScDIT8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the CSV file in write mode\n",
        "with open(csv_file, \"w\", newline=\"\") as file:\n",
        "    # Create a CSV writer object\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the column headers to the CSV file\n",
        "    writer.writerow([\"Mask\", \"Image\"])\n",
        "\n",
        "    # Write each mask and image path to a new row in the CSV file\n",
        "    for mask_path, image_path in zip(test_masks, test_images):\n",
        "        writer.writerow([mask_path, image_path])\n"
      ],
      "metadata": {
        "id": "mi6yGt1vOcTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/research/next_final.h5\", custom_objects={\"dice_coef_loss\": dice_coef_loss, \"dice_coef\": dice_coef})"
      ],
      "metadata": {
        "id": "pjuPSWIIETry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "masks = []\n",
        "images = []\n",
        "with open('/content/drive/MyDrive/research/save_test.csv', 'r') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "      masks.append(row['Mask'])\n",
        "      images.append(row['Image'])"
      ],
      "metadata": {
        "id": "hXddDtzC-KBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_image = []\n",
        "list_mask = []\n",
        "for image in images:\n",
        "  img = np.load(image)\n",
        "  img = cv2.resize(img, (128, 128))\n",
        "  img = tf.expand_dims(img, axis = -1)\n",
        "  list_image.append(img)\n",
        "\n",
        "for mask in masks:\n",
        "  msk = np.load(mask)\n",
        "  msk = cv2.resize(msk, (128, 128))\n",
        "  msk = one_hot_encode(msk)\n",
        "  list_mask.append(msk)"
      ],
      "metadata": {
        "id": "T2uIUa_7A9ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_image = np.array(list_image)\n",
        "list_mask = np.array(list_mask)\n",
        "print(list_image.shape)\n",
        "print(list_mask.shape)"
      ],
      "metadata": {
        "id": "RnvLmf6qtvV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_predict = model.predict(list_image)\n",
        "list_predict = np.round(list_predict)\n",
        "print(list_predict.shape)"
      ],
      "metadata": {
        "id": "BiCbTmzcsvCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 10))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(list_image[14, :, :, :])\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(list_predict[14, :, :, :])\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(list_mask[14, :, :, :])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OQIEteaOZAGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate using confusion_matrix for tumor and pancreas\n",
        "def confusion_matrix_(num_channel, mask, pred):\n",
        "  channel_true = mask[:, :, num_channel]\n",
        "  channel_pred = pred[:, :, num_channel]\n",
        "  y_true = channel_true.flatten()\n",
        "  y_pred = channel_pred.flatten()\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  TP = cm[0, 0]\n",
        "  FP = cm[0, 1]\n",
        "  FN = cm[1, 0]\n",
        "  TN = cm[1, 1]\n",
        "  return TP, TN, FP, FN"
      ],
      "metadata": {
        "id": "NeqvvvXAD0Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP_ARR = []\n",
        "TN_ARR = []\n",
        "FP_ARR = []\n",
        "FN_ARR = []\n",
        "for i in range(list_mask.shape[0]):\n",
        "  mask = list_mask[i, :, :, :]\n",
        "  pred = list_predict[i, :, :, :]\n",
        "  TP, TN, FP, FN = confusion_matrix_(1, mask, pred)\n",
        "  TP_ARR.append(TP)\n",
        "  TN_ARR.append(TN)\n",
        "  FP_ARR.append(FP)\n",
        "  FN_ARR.append(FN)"
      ],
      "metadata": {
        "id": "MdNDzAc9dcCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Dice_similarity_coefficient(TP, FP, FN):\n",
        "  DSC = 2 * TP / (2 * TP + FP + FN)\n",
        "  return DSC\n",
        "def Sensitive(TP, FN):\n",
        "  SEN = TP / (TP + FN)\n",
        "  return SEN\n",
        "def Accuracy(TP, TN, FP, FN):\n",
        "  Acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "  return Acc\n",
        "def Precision(TP, FP):\n",
        "  Pre = TP / (TP + FP)\n",
        "  return Pre\n",
        "def mean_IOU_score(TP, FP, FN):\n",
        "  Mean_Iou = TP / (TP + FP + FN)\n",
        "  return Mean_Iou"
      ],
      "metadata": {
        "id": "7VWgo928bFyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5, 15):\n",
        "  TP, TN, FP, FN = confusion_matrix_(1, list_mask[i], list_predict[i])\n",
        "  print(\"Mean IOU\" + str(i) + \" = \", Dice_similarity_coefficient(TP, FP, FN))\n",
        "  #print(\"Mean IOU\" + str(i) + \" = \",mean_IOU_score(TP, FP, FN))"
      ],
      "metadata": {
        "id": "qBgRwafEcX3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP = np.mean(TP_ARR)\n",
        "TN = np.mean(TN_ARR)\n",
        "FP = np.mean(FP_ARR)\n",
        "FN = np.mean(FN_ARR)\n",
        "Dice_average = Dice_similarity_coefficient(TP, FP, FN)\n",
        "Sen = Sensitive(TP, FN)\n",
        "Acc = Accuracy(TP, TN, FP, FN)\n",
        "Pre = Precision(TP, FP)\n",
        "Iou = mean_IOU_score(TP, FP, FN)\n",
        "print('Dice_average_of_pancreas = ', Dice_average)\n",
        "print('Sensitive_of_pancreas = ', Sen)\n",
        "print('Accuracy_of_pancreas = ', Acc)\n",
        "print('Precision_of_pancreas = ', Pre)\n",
        "print('Mean IOU of pancreas', Iou)"
      ],
      "metadata": {
        "id": "TPCFEVOvcuJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP_ARR_1 = []\n",
        "TN_ARR_1 = []\n",
        "FP_ARR_1 = []\n",
        "FN_ARR_1 = []\n",
        "for i in range(list_mask.shape[0]):\n",
        "  mask = list_mask[i, :, :, :]\n",
        "  pred = list_predict[i, :, :, :]\n",
        "  TP, TN, FP, FN = confusion_matrix_(2, mask, pred)\n",
        "  TP_ARR_1.append(TP)\n",
        "  TN_ARR_1.append(TN)\n",
        "  FP_ARR_1.append(FP)\n",
        "  FN_ARR_1.append(FN)"
      ],
      "metadata": {
        "id": "oTFjDCgReEbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP_1 = np.mean(TP_ARR_1)\n",
        "TN_1 = np.mean(TN_ARR_1)\n",
        "FP_1 = np.mean(FP_ARR_1)\n",
        "FN_1 = np.mean(FN_ARR_1)\n",
        "Dice_average_1 = Dice_similarity_coefficient(TP_1, FP_1, FN_1)\n",
        "Sen_1 = Sensitive(TP_1, FN_1)\n",
        "Acc_1 = Accuracy(TP_1, TN_1, FP_1, FN_1)\n",
        "Pre_1 = Precision(TP_1, FP_1)\n",
        "Iou_1 = mean_IOU_score(TP_1, FP_1, FN_1)\n",
        "print('Dice_avegare_of_tumor = ', Dice_average_1)\n",
        "print('Sensitive_of_tumor = ', Sen_1)\n",
        "print('Accuracy_of_tumor = ', Acc_1)\n",
        "print('Precision_of_tumor = ', Pre_1)\n",
        "print('Mean IOU of tumor', Iou_1)"
      ],
      "metadata": {
        "id": "ZC4oUrAteXB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}